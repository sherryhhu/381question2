{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3206ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2 in /Users/sherryhu/anaconda3/lib/python3.11/site-packages (1.24.3)\n",
      "Requirement already satisfied: matplotlib in /Users/sherryhu/anaconda3/lib/python3.11/site-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/sherryhu/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/sherryhu/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/sherryhu/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/sherryhu/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sherryhu/anaconda3/lib/python3.11/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/sherryhu/anaconda3/lib/python3.11/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/sherryhu/anaconda3/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/sherryhu/anaconda3/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/sherryhu/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"numpy<2\" matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71119044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Enable inline plotting for Jupyter Notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4001de11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 193 CSV files to load.\n",
      "\n",
      "--- Combined DataFrame ---\n",
      "Total rows: 206216\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 206216 entries, 0 to 206215\n",
      "Columns: 807 entries, site_info.monkey to source_file\n",
      "dtypes: int64(802), object(5)\n",
      "memory usage: 1.2+ GB\n",
      "None\n",
      "\n",
      "First 5 rows of combined data:\n",
      "  site_info.monkey site_info.region  labels.stimID  labels.person  \\\n",
      "0             bert               am              1              1   \n",
      "1             bert               am              1              1   \n",
      "2             bert               am              1              1   \n",
      "3             bert               am              1              1   \n",
      "4             bert               am              1              1   \n",
      "\n",
      "  labels.orientation labels.orient_person_combo  time.1_2  time.2_3  time.3_4  \\\n",
      "0              front                    front 1         0         0         0   \n",
      "1              front                    front 1         0         0         0   \n",
      "2              front                    front 1         0         0         0   \n",
      "3              front                    front 1         0         0         0   \n",
      "4              front                    front 1         0         0         0   \n",
      "\n",
      "   time.4_5  ...  time.792_793  time.793_794  time.794_795  time.795_796  \\\n",
      "0         0  ...             0             0             0             0   \n",
      "1         0  ...             0             0             0             0   \n",
      "2         0  ...             0             0             0             0   \n",
      "3         0  ...             0             0             0             0   \n",
      "4         0  ...             0             0             0             0   \n",
      "\n",
      "   time.796_797  time.797_798  time.798_799  time.799_800  time.800_801  \\\n",
      "0             0             0             0             0             0   \n",
      "1             0             0             0             0             0   \n",
      "2             0             0             0             0             0   \n",
      "3             0             0             0             0             0   \n",
      "4             0             0             0             0             0   \n",
      "\n",
      "                       source_file  \n",
      "0  raster_data_bert_am_site070.csv  \n",
      "1  raster_data_bert_am_site070.csv  \n",
      "2  raster_data_bert_am_site070.csv  \n",
      "3  raster_data_bert_am_site070.csv  \n",
      "4  raster_data_bert_am_site070.csv  \n",
      "\n",
      "[5 rows x 807 columns]\n",
      "Checking unique orientations in the originally loaded combined_df:\n",
      "['front' 'left 3/4' 'left profile' 'right 3/4' 'right profile' 'up' 'down'\n",
      " 'back']\n",
      "\n",
      "Value counts for orientation:\n",
      "right profile    26160\n",
      "back             26024\n",
      "up               25757\n",
      "left profile     25698\n",
      "front            25691\n",
      "right 3/4        25658\n",
      "left 3/4         25648\n",
      "down             25580\n",
      "Name: labels.orientation, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset folder path (as you did)\n",
    "folder_path = \"Freiwald_Tsao_faceviews_AM_data_csv\"\n",
    "\n",
    "# Use glob to find all CSV files in the folder\n",
    "# The pattern ensures you only get files ending with .csv\n",
    "all_csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "# Check if any files were found\n",
    "if not all_csv_files:\n",
    "    print(f\"Error: No CSV files found in the folder: {folder_path}\")\n",
    "else:\n",
    "    print(f\"Found {len(all_csv_files)} CSV files to load.\")\n",
    "\n",
    "    # Create an empty list to hold the individual DataFrames\n",
    "    list_of_dfs = []\n",
    "\n",
    "    # Loop through the list of found CSV files\n",
    "    for file_path in all_csv_files:\n",
    "        try:\n",
    "            # Read each CSV file into a DataFrame\n",
    "            df_single = pd.read_csv(file_path)\n",
    "            # Optional: Add a column to know which file the data came from\n",
    "            df_single['source_file'] = os.path.basename(file_path)\n",
    "            # Append the DataFrame to the list\n",
    "            list_of_dfs.append(df_single)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {file_path}: {e}\")\n",
    "\n",
    "    # Check if any DataFrames were successfully loaded\n",
    "    if not list_of_dfs:\n",
    "        print(\"No dataframes were loaded successfully.\")\n",
    "    else:\n",
    "        # Concatenate all DataFrames in the list into a single DataFrame\n",
    "        combined_df = pd.concat(list_of_dfs, axis=0, ignore_index=True)\n",
    "\n",
    "        # Display info about the combined DataFrame\n",
    "        print(\"\\n--- Combined DataFrame ---\")\n",
    "        print(f\"Total rows: {len(combined_df)}\")\n",
    "        print(combined_df.info())\n",
    "        print(\"\\nFirst 5 rows of combined data:\")\n",
    "        print(combined_df.head())\n",
    "        # Optional: Check unique values in key columns again\n",
    "        # print(\"\\nUnique persons in combined data:\", combined_df['labels.person'].unique())\n",
    "        # print(\"Unique orientations in combined data:\", combined_df['labels.orientation'].unique())\n",
    "        # (Run this right after creating combined_df)\n",
    "        print(\"Checking unique orientations in the originally loaded combined_df:\")\n",
    "        print(combined_df['labels.orientation'].unique())\n",
    "        print(\"\\nValue counts for orientation:\")\n",
    "        print(combined_df['labels.orientation'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dff82b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique orientations confirmed: ['front' 'left 3/4' 'left profile' 'right 3/4' 'right profile' 'up' 'down'\n",
      " 'back']\n",
      "\n",
      "Using angle mapping: {'front': 0.0, 'left 3/4': -45.0, 'left profile': -90.0, 'right 3/4': 45.0, 'right profile': 90.0, 'up': 0.0, 'down': 0.0, 'back': 180.0}\n",
      "\n",
      "Target variable 'orientation_angle' created. Shape: (206216,)\n",
      "count    206216.000000\n",
      "mean         22.919415\n",
      "std          78.129978\n",
      "min         -90.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%          90.000000\n",
      "max         180.000000\n",
      "Name: orientation_angle, dtype: float64\n",
      "\n",
      "Angle value counts:\n",
      "-90.0     25698\n",
      "-45.0     25648\n",
      " 0.0      77028\n",
      " 45.0     25658\n",
      " 90.0     26160\n",
      " 180.0    26024\n",
      "Name: orientation_angle, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# (Run this in your Jupyter Notebook - Cell after loading combined_df)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Define Orientation to Angle Mapping ---\n",
    "# Using the confirmed unique orientations\n",
    "unique_orientations = combined_df['labels.orientation'].unique()\n",
    "print(f\"Unique orientations confirmed: {unique_orientations}\")\n",
    "\n",
    "# Define the COMPLETE mapping\n",
    "orientation_to_angle = {\n",
    "    'front': 0.0,\n",
    "    'left 3/4': -45.0,\n",
    "    'left profile': -90.0,\n",
    "    'right 3/4': 45.0,\n",
    "    'right profile': 90.0,\n",
    "    'up': 0.0,       # Treating as 0 angle - adjust if needed\n",
    "    'down': 0.0,     # Treating as 0 angle - adjust if needed\n",
    "    'back': 180.0    # Adjust if needed\n",
    "}\n",
    "# Ensure all found orientations are in the map\n",
    "orientation_to_angle = {k: v for k, v in orientation_to_angle.items() if k in unique_orientations}\n",
    "print(f\"\\nUsing angle mapping: {orientation_to_angle}\")\n",
    "\n",
    "# --- Create the target variable column ---\n",
    "# Apply mapping directly to the original combined_df\n",
    "combined_df['orientation_angle'] = combined_df['labels.orientation'].map(orientation_to_angle)\n",
    "\n",
    "# --- Handle potential NaNs (though unlikely now) and select target Y ---\n",
    "original_rows = len(combined_df)\n",
    "combined_df.dropna(subset=['orientation_angle'], inplace=True)\n",
    "rows_after_drop = len(combined_df)\n",
    "if (original_rows - rows_after_drop) > 0:\n",
    "    print(f\"\\nWarning: Dropped {original_rows - rows_after_drop} rows with NaN angles.\")\n",
    "\n",
    "# Define the target variable Y\n",
    "Y_angle = combined_df['orientation_angle'].copy()\n",
    "print(f\"\\nTarget variable 'orientation_angle' created. Shape: {Y_angle.shape}\")\n",
    "print(Y_angle.describe())\n",
    "print(f\"\\nAngle value counts:\\n{Y_angle.value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65bea094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Engineering Features: Firing Rates in Bins ---\n",
      "Using 8 bins of size 50ms.\n",
      "Calculating firing rates per bin...\n",
      "\n",
      "Firing rate features calculated.\n",
      "Shape of pattern features X_patterns: (206216, 8)\n",
      "Final aligned shapes: X=(206216, 8), Y=(206216,)\n"
     ]
    }
   ],
   "source": [
    "# (Run this in your Jupyter Notebook - Cell after angle mapping)\n",
    "# Assumes 'combined_df' has 'orientation_angle' and Y_angle exists\n",
    "\n",
    "print(\"--- Engineering Features: Firing Rates in Bins ---\")\n",
    "\n",
    "# --- Define Time Bins ---\n",
    "bin_size_ms = 50  # 50ms bins\n",
    "window_ms = 400   # First 400ms\n",
    "n_bins = window_ms // bin_size_ms\n",
    "bin_edges = np.arange(0, window_ms + 1, bin_size_ms)\n",
    "bin_duration_s = bin_size_ms / 1000.0\n",
    "print(f\"Using {n_bins} bins of size {bin_size_ms}ms.\")\n",
    "\n",
    "# --- Identify Time Columns ---\n",
    "time_cols_pattern = [f'time.{i}_{i+1}' for i in range(1, window_ms)]\n",
    "time_cols_pattern = [col for col in time_cols_pattern if col in combined_df.columns]\n",
    "\n",
    "if not time_cols_pattern:\n",
    "    print(\"Error: Could not find relevant time columns.\")\n",
    "else:\n",
    "    # Select spike data corresponding to rows with valid angles (using Y_angle.index)\n",
    "    spike_data = combined_df.loc[Y_angle.index, time_cols_pattern]\n",
    "\n",
    "    # --- Calculate Rates ---\n",
    "    rate_feature_names = [f'rate_bin_{i*bin_size_ms}_{(i+1)*bin_size_ms}ms' for i in range(n_bins)]\n",
    "    X_patterns = pd.DataFrame(index=spike_data.index, columns=rate_feature_names, dtype=float)\n",
    "\n",
    "    print(\"Calculating firing rates per bin...\")\n",
    "    for i in range(n_bins):\n",
    "        start_col_idx = bin_edges[i]\n",
    "        end_col_idx = bin_edges[i+1]\n",
    "        cols_in_bin = [f'time.{t}_{t+1}' for t in range(start_col_idx + 1, end_col_idx + 1)]\n",
    "        cols_in_bin = [col for col in cols_in_bin if col in spike_data.columns]\n",
    "\n",
    "        if cols_in_bin:\n",
    "            X_patterns[rate_feature_names[i]] = spike_data[cols_in_bin].sum(axis=1) / bin_duration_s\n",
    "        else:\n",
    "            X_patterns[rate_feature_names[i]] = 0.0\n",
    "\n",
    "    print(\"\\nFiring rate features calculated.\")\n",
    "    print(f\"Shape of pattern features X_patterns: {X_patterns.shape}\")\n",
    "    # print(X_patterns.head()) # Optional: view features\n",
    "\n",
    "    # Ensure X and Y are finally aligned\n",
    "    X_patterns.dropna(inplace=True) # Drop rows if any calculation failed\n",
    "    Y_angle = Y_angle.loc[X_patterns.index]\n",
    "    print(f\"Final aligned shapes: X={X_patterns.shape}, Y={Y_angle.shape}\")\n",
    "\n",
    "    # Optional memory cleanup\n",
    "    # import gc\n",
    "    # del spike_data\n",
    "    # gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b0fa777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing Data: Splitting and Scaling ---\n",
      "Split data: X_train=(144351, 8), X_test=(61865, 8)\n",
      "Scaling features using StandardScaler...\n",
      "Scaling complete.\n"
     ]
    }
   ],
   "source": [
    "# (Run this in your Jupyter Notebook - Cell after feature engineering)\n",
    "# Assumes X_patterns and Y_angle exist and are aligned\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"\\n--- Preparing Data: Splitting and Scaling ---\")\n",
    "\n",
    "# Split data\n",
    "X_train_pat, X_test_pat, Y_train_ang, Y_test_ang = train_test_split(\n",
    "    X_patterns, Y_angle, test_size=0.3, random_state=42\n",
    ")\n",
    "print(f\"Split data: X_train={X_train_pat.shape}, X_test={X_test_pat.shape}\")\n",
    "\n",
    "# Scale features\n",
    "print(\"Scaling features using StandardScaler...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_pat)\n",
    "X_test_scaled = scaler.transform(X_test_pat)\n",
    "print(\"Scaling complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fef8a9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Ridge Regression (Predicting Angle from Patterns) ---\n",
      "Best alpha found: 10000\n",
      "Best CV R2 score: 0.0013\n",
      "\n",
      "--- Ridge Regression (Angle Prediction - Test Data Eval) ---\n",
      "Test MSE: 6091.9345\n",
      "Test MAE: 62.0856 (degrees, approx)\n",
      "Test R-squared: 0.0016\n"
     ]
    }
   ],
   "source": [
    "# (Run this in your Jupyter Notebook - Cell after split/scale)\n",
    "# Assumes X_train_scaled, Y_train_ang, X_test_scaled, Y_test_ang exist\n",
    "\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Training Ridge Regression (Predicting Angle from Patterns) ---\")\n",
    "alphas_ridge = [1, 10, 100, 1000, 10000, 100000] # Wider range for scaled data\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# Use R2 scoring, as MAE/MSE depend on angle units (degrees)\n",
    "ridge_cv_angle = RidgeCV(alphas=alphas_ridge, cv=kf, scoring='r2')\n",
    "ridge_cv_angle.fit(X_train_scaled, Y_train_ang)\n",
    "\n",
    "best_alpha_ridge_angle = ridge_cv_angle.alpha_\n",
    "print(f\"Best alpha found: {best_alpha_ridge_angle}\")\n",
    "print(f\"Best CV R2 score: {ridge_cv_angle.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on Test Set\n",
    "Y_pred_ridge_angle = ridge_cv_angle.predict(X_test_scaled)\n",
    "mse_ridge_angle = mean_squared_error(Y_test_ang, Y_pred_ridge_angle)\n",
    "mae_ridge_angle = mean_absolute_error(Y_test_ang, Y_pred_ridge_angle)\n",
    "r2_ridge_angle = r2_score(Y_test_ang, Y_pred_ridge_angle)\n",
    "\n",
    "print(\"\\n--- Ridge Regression (Angle Prediction - Test Data Eval) ---\")\n",
    "print(f\"Test MSE: {mse_ridge_angle:.4f}\")\n",
    "print(f\"Test MAE: {mae_ridge_angle:.4f} (degrees, approx)\")\n",
    "print(f\"Test R-squared: {r2_ridge_angle:.4f}\") # <-- This is the key performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db0a675f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Lasso Regression (Predicting Angle from Patterns) ---\n",
      "Best alpha found: 0.1\n",
      "\n",
      "--- Lasso Regression (Angle Prediction - Test Data Eval) ---\n",
      "Test MSE: 6091.9527\n",
      "Test MAE: 62.0870 (degrees, approx)\n",
      "Test R-squared: 0.0016\n",
      "\n",
      "--- Lasso Coefficients (Angle Prediction) ---\n",
      "Number of features used (non-zero coefficients): 6 out of 8\n"
     ]
    }
   ],
   "source": [
    "# --- Cell: Lasso Regression (Predicting Angle from Patterns) - CORRECTED ---\n",
    "# (Run this in your Jupyter Notebook)\n",
    "# Assumes X_train_scaled, Y_train_ang, X_test_scaled, Y_test_ang exist\n",
    "# Assumes X_patterns DataFrame exists for feature names\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Training Lasso Regression (Predicting Angle from Patterns) ---\")\n",
    "alphas_lasso = [0.001, 0.01, 0.1, 1.0, 10.0, 20.0] # Adjust range if needed\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# *** CORRECTION: Removed scoring='r2' argument ***\n",
    "lasso_cv_angle = LassoCV(alphas=alphas_lasso, cv=kf, random_state=42, max_iter=5000, n_jobs=-1, verbose=0)\n",
    "\n",
    "lasso_cv_angle.fit(X_train_scaled, Y_train_ang)\n",
    "\n",
    "best_alpha_lasso_angle = lasso_cv_angle.alpha_\n",
    "print(f\"Best alpha found: {best_alpha_lasso_angle}\")\n",
    "\n",
    "# Evaluate on Test Set\n",
    "Y_pred_lasso_angle = lasso_cv_angle.predict(X_test_scaled)\n",
    "mse_lasso_angle = mean_squared_error(Y_test_ang, Y_pred_lasso_angle)\n",
    "mae_lasso_angle = mean_absolute_error(Y_test_ang, Y_pred_lasso_angle)\n",
    "r2_lasso_angle = r2_score(Y_test_ang, Y_pred_lasso_angle)\n",
    "\n",
    "print(\"\\n--- Lasso Regression (Angle Prediction - Test Data Eval) ---\")\n",
    "print(f\"Test MSE: {mse_lasso_angle:.4f}\")\n",
    "print(f\"Test MAE: {mae_lasso_angle:.4f} (degrees, approx)\")\n",
    "print(f\"Test R-squared: {r2_lasso_angle:.4f}\") # <-- Key performance metric\n",
    "\n",
    "# Examine Coefficients (Weights on firing rates in different time bins)\n",
    "print(\"\\n--- Lasso Coefficients (Angle Prediction) ---\")\n",
    "try:\n",
    "    # Assumes X_patterns is the dataframe used to generate X_train_pat/X_test_pat before scaling\n",
    "    coefficients_lasso_angle = pd.Series(lasso_cv_angle.coef_, index=X_patterns.columns)\n",
    "    num_nonzero_coeffs_angle = (coefficients_lasso_angle != 0).sum()\n",
    "    print(f\"Number of features used (non-zero coefficients): {num_nonzero_coeffs_angle} out of {len(coefficients_lasso_angle)}\")\n",
    "    # print(\"Non-zero coefficients:\")\n",
    "    # print(coefficients_lasso_angle[coefficients_lasso_angle != 0].sort_values())\n",
    "except NameError:\n",
    "    print(\"Could not display coefficients (original feature names not found).\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred displaying coefficients: {e}\")\n",
    "    print(f\"An error occurred displaying coefficients: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4457fc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Tuned Random Forest (Predicting Angle from Patterns) ---\n",
      "Starting Randomized Search for Random Forest...\n",
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n",
      "Best parameters found: {'max_depth': None, 'min_samples_leaf': 17, 'min_samples_split': 23, 'n_estimators': 157}\n",
      "Best CV R-squared score: -0.0015\n",
      "\n",
      "--- Tuned Random Forest (Angle Prediction - Test Data Eval) ---\n",
      "Test MSE: 6101.4221\n",
      "Test MAE: 61.9841 (degrees, approx)\n",
      "Test R-squared: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# --- Cell: Tuned Random Forest (Predicting Angle from Patterns) ---\n",
    "# (Run this in your Jupyter Notebook)\n",
    "# Assumes X_train_scaled, Y_train_ang, X_test_scaled, Y_test_ang exist\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import randint\n",
    "import numpy as np # Ensure numpy is imported if running standalone\n",
    "\n",
    "print(\"\\n--- Training Tuned Random Forest (Predicting Angle from Patterns) ---\")\n",
    "# Define parameter distribution\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 200),        # Number of trees\n",
    "    'max_depth': [10, 15, 20, 25, 30, None], # Test specific depths + None\n",
    "    'min_samples_split': randint(5, 30),     # Min samples to split (adjusted)\n",
    "    'min_samples_leaf': randint(3, 20)       # Min samples per leaf (adjusted)\n",
    "}\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Initialize Randomized Search\n",
    "# Adjust n_iter or cv if needed for your system's performance\n",
    "random_search_angle = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=15, # Number of parameter settings sampled (adjust as needed)\n",
    "    cv=3,      # Use 3-fold CV (adjust as needed)\n",
    "    scoring='r2', # Optimize for R-squared\n",
    "    random_state=42,\n",
    "    n_jobs=-1, # Use all cores for CV folds\n",
    "    verbose=1  # Show progress\n",
    ")\n",
    "\n",
    "# Fit Randomized Search on the scaled training data\n",
    "print(\"Starting Randomized Search for Random Forest...\")\n",
    "random_search_angle.fit(X_train_scaled, Y_train_ang)\n",
    "\n",
    "print(f\"Best parameters found: {random_search_angle.best_params_}\")\n",
    "print(f\"Best CV R-squared score: {random_search_angle.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate the best estimator found on the test set\n",
    "best_rf_model_angle = random_search_angle.best_estimator_\n",
    "Y_pred_rf_tuned_angle = best_rf_model_angle.predict(X_test_scaled)\n",
    "r2_rf_tuned_angle = r2_score(Y_test_ang, Y_pred_rf_tuned_angle)\n",
    "mse_rf_tuned_angle = mean_squared_error(Y_test_ang, Y_pred_rf_tuned_angle)\n",
    "mae_rf_tuned_angle = mean_absolute_error(Y_test_ang, Y_pred_rf_tuned_angle)\n",
    "\n",
    "print(\"\\n--- Tuned Random Forest (Angle Prediction - Test Data Eval) ---\")\n",
    "print(f\"Test MSE: {mse_rf_tuned_angle:.4f}\")\n",
    "print(f\"Test MAE: {mae_rf_tuned_angle:.4f} (degrees, approx)\")\n",
    "print(f\"Test R-squared: {r2_rf_tuned_angle:.4f}\") # <-- Key performance metric\n",
    "\n",
    "# Optional: Feature Importances\n",
    "# import pandas as pd\n",
    "# try:\n",
    "#     importances_rf_angle = pd.Series(best_rf_model_angle.feature_importances_, index=X_patterns.columns)\n",
    "#     print(\"\\n--- Random Forest Feature Importances (Angle Prediction) ---\")\n",
    "#     print(\"Top Features:\")\n",
    "#     print(importances_rf_angle.sort_values(ascending=False).head(10))\n",
    "# except NameError:\n",
    "#      print(\"\\nCould not display feature importances (original feature names not found).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3734f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Tuned SVR (Predicting Angle from Patterns) ---\n",
      "Starting Randomized Search for SVR...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "# --- Cell: SVR with Randomized Search (Predicting Angle from Patterns) ---\n",
    "# (Run this in your Jupyter Notebook)\n",
    "# Assumes X_train_scaled, Y_train_ang, X_test_scaled, Y_test_ang exist\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import uniform, loguniform # For parameter distributions\n",
    "import numpy as np # Ensure numpy is imported\n",
    "\n",
    "print(\"\\n--- Training Tuned SVR (Predicting Angle from Patterns) ---\")\n",
    "\n",
    "# Define parameter distribution for Randomized Search\n",
    "# Using RBF kernel by default, common for SVR\n",
    "param_dist_svr = {\n",
    "    'C': loguniform(1e-1, 1e3),      # Regularization parameter (log scale)\n",
    "    'gamma': loguniform(1e-4, 1e-1), # Kernel coefficient (log scale) - sensitive!\n",
    "    'epsilon': uniform(0.01, 0.5)    # Margin of tolerance\n",
    "}\n",
    "svr = SVR(kernel='rbf') # Specify RBF kernel\n",
    "\n",
    "# Initialize Randomized Search\n",
    "# Adjust n_iter or cv if needed for your system's performance\n",
    "# SVR tuning can be slow! Start with low n_iter.\n",
    "random_search_svr = RandomizedSearchCV(\n",
    "    estimator=svr,\n",
    "    param_distributions=param_dist_svr,\n",
    "    n_iter=10, # Low number of iterations due to potential slowness\n",
    "    cv=3,      # 3-fold CV\n",
    "    scoring='r2', # Optimize for R-squared\n",
    "    random_state=42,\n",
    "    n_jobs=-1, # Use all cores for CV folds\n",
    "    verbose=1  # Show progress\n",
    ")\n",
    "\n",
    "# Fit Randomized Search on the scaled training data\n",
    "print(\"Starting Randomized Search for SVR...\")\n",
    "random_search_svr.fit(X_train_scaled, Y_train_ang)\n",
    "\n",
    "print(f\"Best parameters found: {random_search_svr.best_params_}\")\n",
    "print(f\"Best CV R-squared score: {random_search_svr.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate the best estimator found on the test set\n",
    "best_svr_model = random_search_svr.best_estimator_\n",
    "Y_pred_svr_tuned = best_svr_model.predict(X_test_scaled)\n",
    "r2_svr_tuned = r2_score(Y_test_ang, Y_pred_svr_tuned)\n",
    "mse_svr_tuned = mean_squared_error(Y_test_ang, Y_pred_svr_tuned)\n",
    "mae_svr_tuned = mean_absolute_error(Y_test_ang, Y_pred_svr_tuned)\n",
    "\n",
    "print(\"\\n--- Tuned SVR (Angle Prediction - Test Data Eval) ---\")\n",
    "print(f\"Test MSE: {mse_svr_tuned:.4f}\")\n",
    "print(f\"Test MAE: {mae_svr_tuned:.4f} (degrees, approx)\")\n",
    "print(f\"Test R-squared: {r2_svr_tuned:.4f}\") # <-- Key performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c27796",
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
