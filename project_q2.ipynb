{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d3bbdcb-59e2-4925-87fc-b3850a36084c",
   "metadata": {},
   "source": [
    "## 2. Predict Total Number of Spikes Across All Neurons\n",
    "\n",
    "####  Task: Given the identity/orientation of the face, predict the total number of spikes during a trial.\n",
    "####  Goal: Test how much a stimulus drives global activity.\n",
    "####  Fun twist: Could reveal which identities cause stronger overall brain activation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44812568-1534-4e7b-97c6-32b9caeb691f",
   "metadata": {},
   "source": [
    "- Inputs: face identities(1-25), head orientations(1-8), encoding variables\n",
    "- Outputs: total spikes (continuous)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15543e7f-0f3c-4cc1-a493-b5c2a7c57149",
   "metadata": {},
   "source": [
    "Methods: \n",
    "1. Linear regression / multi- linear regression\n",
    "2. lasso/ ridge\n",
    "3. Random forest\n",
    "4. PCR\n",
    "Evaluation: k-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bedcd424-ed32-4678-b544-9284f735f41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy<2 in ./.local/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in ./.local/lib/python3.11/site-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/miniconda3/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/miniconda3/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/miniconda3/lib/python3.11/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/miniconda3/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/lib/python3.11/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/miniconda3/lib/python3.11/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/miniconda3/lib/python3.11/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/miniconda3/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"numpy<2\" matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05319ca7-da25-4799-8514-30bc5b6df86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Enable inline plotting for Jupyter Notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62a417d1-8f35-460a-981c-a18137cd57c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded: Freiwald_Tsao_faceviews_AM_data_csv/raster_data_bert_am_site070.csv\n",
      "  site_info.monkey site_info.region  labels.stimID  labels.person  \\\n",
      "0             bert               am              1              1   \n",
      "1             bert               am              1              1   \n",
      "2             bert               am              1              1   \n",
      "3             bert               am              1              1   \n",
      "4             bert               am              1              1   \n",
      "\n",
      "  labels.orientation labels.orient_person_combo  time.1_2  time.2_3  time.3_4  \\\n",
      "0              front                    front 1         0         0         0   \n",
      "1              front                    front 1         0         0         0   \n",
      "2              front                    front 1         0         0         0   \n",
      "3              front                    front 1         0         0         0   \n",
      "4              front                    front 1         0         0         0   \n",
      "\n",
      "   time.4_5  ...  time.791_792  time.792_793  time.793_794  time.794_795  \\\n",
      "0         0  ...             0             0             0             0   \n",
      "1         0  ...             0             0             0             0   \n",
      "2         0  ...             0             0             0             0   \n",
      "3         0  ...             0             0             0             0   \n",
      "4         0  ...             0             0             0             0   \n",
      "\n",
      "   time.795_796  time.796_797  time.797_798  time.798_799  time.799_800  \\\n",
      "0             0             0             0             0             0   \n",
      "1             0             0             0             0             0   \n",
      "2             0             0             0             0             0   \n",
      "3             0             0             0             0             0   \n",
      "4             0             0             0             0             0   \n",
      "\n",
      "   time.800_801  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n",
      "\n",
      "[5 rows x 806 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600 entries, 0 to 1599\n",
      "Columns: 806 entries, site_info.monkey to time.800_801\n",
      "dtypes: int64(802), object(4)\n",
      "memory usage: 9.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset folder path\n",
    "folder_path = \"Freiwald_Tsao_faceviews_AM_data_csv\"\n",
    "\n",
    "# List all CSV files\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Load one sample file to inspect the structure\n",
    "sample_file = os.path.join(folder_path, csv_files[0])  # First CSV file\n",
    "df = pd.read_csv(sample_file)\n",
    "\n",
    "# Display info\n",
    "print(\"Dataset Loaded:\", sample_file)\n",
    "print(df.head())  # Show first few rows\n",
    "print(df.info())  # Column details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "681bc4e0-0665-4177-bfc0-372383d8b75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 193 CSV files to load.\n",
      "\n",
      "--- Combined DataFrame ---\n",
      "Total rows: 206216\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 206216 entries, 0 to 206215\n",
      "Columns: 807 entries, site_info.monkey to source_file\n",
      "dtypes: int64(802), object(5)\n",
      "memory usage: 1.2+ GB\n",
      "None\n",
      "\n",
      "First 5 rows of combined data:\n",
      "  site_info.monkey site_info.region  labels.stimID  labels.person  \\\n",
      "0             bert               am              1              1   \n",
      "1             bert               am              1              1   \n",
      "2             bert               am              1              1   \n",
      "3             bert               am              1              1   \n",
      "4             bert               am              1              1   \n",
      "\n",
      "  labels.orientation labels.orient_person_combo  time.1_2  time.2_3  time.3_4  \\\n",
      "0              front                    front 1         0         0         0   \n",
      "1              front                    front 1         0         0         0   \n",
      "2              front                    front 1         0         0         0   \n",
      "3              front                    front 1         0         0         0   \n",
      "4              front                    front 1         0         0         0   \n",
      "\n",
      "   time.4_5  ...  time.792_793  time.793_794  time.794_795  time.795_796  \\\n",
      "0         0  ...             0             0             0             0   \n",
      "1         0  ...             0             0             0             0   \n",
      "2         0  ...             0             0             0             0   \n",
      "3         0  ...             0             0             0             0   \n",
      "4         0  ...             0             0             0             0   \n",
      "\n",
      "   time.796_797  time.797_798  time.798_799  time.799_800  time.800_801  \\\n",
      "0             0             0             0             0             0   \n",
      "1             0             0             0             0             0   \n",
      "2             0             0             0             0             0   \n",
      "3             0             0             0             0             0   \n",
      "4             0             0             0             0             0   \n",
      "\n",
      "                       source_file  \n",
      "0  raster_data_bert_am_site070.csv  \n",
      "1  raster_data_bert_am_site070.csv  \n",
      "2  raster_data_bert_am_site070.csv  \n",
      "3  raster_data_bert_am_site070.csv  \n",
      "4  raster_data_bert_am_site070.csv  \n",
      "\n",
      "[5 rows x 807 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset folder path (as you did)\n",
    "folder_path = \"Freiwald_Tsao_faceviews_AM_data_csv\"\n",
    "\n",
    "# Use glob to find all CSV files in the folder\n",
    "# The pattern ensures you only get files ending with .csv\n",
    "all_csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "# Check if any files were found\n",
    "if not all_csv_files:\n",
    "    print(f\"Error: No CSV files found in the folder: {folder_path}\")\n",
    "else:\n",
    "    print(f\"Found {len(all_csv_files)} CSV files to load.\")\n",
    "\n",
    "    # Create an empty list to hold the individual DataFrames\n",
    "    list_of_dfs = []\n",
    "\n",
    "    # Loop through the list of found CSV files\n",
    "    for file_path in all_csv_files:\n",
    "        try:\n",
    "            # Read each CSV file into a DataFrame\n",
    "            df_single = pd.read_csv(file_path)\n",
    "            # Optional: Add a column to know which file the data came from\n",
    "            df_single['source_file'] = os.path.basename(file_path)\n",
    "            # Append the DataFrame to the list\n",
    "            list_of_dfs.append(df_single)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {file_path}: {e}\")\n",
    "\n",
    "    # Check if any DataFrames were successfully loaded\n",
    "    if not list_of_dfs:\n",
    "        print(\"No dataframes were loaded successfully.\")\n",
    "    else:\n",
    "        # Concatenate all DataFrames in the list into a single DataFrame\n",
    "        combined_df = pd.concat(list_of_dfs, axis=0, ignore_index=True)\n",
    "\n",
    "        # Display info about the combined DataFrame\n",
    "        print(\"\\n--- Combined DataFrame ---\")\n",
    "        print(f\"Total rows: {len(combined_df)}\")\n",
    "        print(combined_df.info())\n",
    "        print(\"\\nFirst 5 rows of combined data:\")\n",
    "        print(combined_df.head())\n",
    "        # Optional: Check unique values in key columns again\n",
    "        # print(\"\\nUnique persons in combined data:\", combined_df['labels.person'].unique())\n",
    "        # print(\"Unique orientations in combined data:\", combined_df['labels.orientation'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9897b855-9f75-4ce5-a689-7b03e8a5f467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating total spikes for the first 400ms...\n",
      "Calculated 'total_spikes_400ms'. Head:\n",
      "0    0\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "Name: total_spikes_400ms, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Assume 'combined_df' is your DataFrame with all loaded data\n",
    "\n",
    "# 1. Calculate Total Spikes (Y) for the first 400ms\n",
    "print(\"Calculating total spikes for the first 400ms...\")\n",
    "# Identify the time columns for the first 400ms\n",
    "time_cols_400ms = [f'time.{i}_{i+1}' for i in range(1, 400)] # Columns time.1_2 up to time.399_400\n",
    "# Filter out columns that might not exist (though unlikely with combined data)\n",
    "time_cols_400ms = [col for col in time_cols_400ms if col in combined_df.columns]\n",
    "\n",
    "if not time_cols_400ms:\n",
    "     print(\"Error: No time columns found in the expected format 'time.X_Y' up to 400ms in combined_df.\")\n",
    "else:\n",
    "    # Calculate the sum across these columns for each row (trial)\n",
    "    combined_df['total_spikes_400ms'] = combined_df[time_cols_400ms].sum(axis=1)\n",
    "    print(f\"Calculated 'total_spikes_400ms'. Head:\\n{combined_df['total_spikes_400ms'].head()}\")\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3a913b1-f6a6-43ac-93d6-b944e6df4cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selecting features and target...\n",
      "Selected features (X_categorical) and target (Y).\n"
     ]
    }
   ],
   "source": [
    "  # 2. Select Predictor Columns (X features) and Target (Y)\n",
    "print(\"\\nSelecting features and target...\")\n",
    "    # Handle potential missing values in label columns if any exist\n",
    "combined_df.dropna(subset=['labels.person', 'labels.orientation'], inplace=True) # Drop rows where labels are missing\n",
    "X_categorical = combined_df[['labels.person', 'labels.orientation']]\n",
    "Y = combined_df['total_spikes_400ms']\n",
    "print(\"Selected features (X_categorical) and target (Y).\")\n",
    "\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de60277f-e06a-43ef-a13d-bfaf0fddbce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoding categorical features...\n",
      "Encoded features shape: (206216, 33)\n"
     ]
    }
   ],
   "source": [
    " # 3. Encode Categorical Predictors\n",
    "print(\"\\nEncoding categorical features...\")\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_encoded = encoder.fit_transform(X_categorical)\n",
    "feature_names = encoder.get_feature_names_out(X_categorical.columns)\n",
    "X_encoded_df = pd.DataFrame(X_encoded, columns=feature_names, index=X_categorical.index) # Use index from X_categorical\n",
    "print(f\"Encoded features shape: {X_encoded_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e71f05c-8b0e-4360-9515-2ed0f3058517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting data into training and testing sets...\n",
      "X_train shape: (144351, 33), Y_train shape: (144351,)\n",
      "X_test shape: (61865, 33), Y_test shape: (61865,)\n"
     ]
    }
   ],
   "source": [
    "# 4. Split Data into Training and Testing Sets\n",
    "print(\"\\nSplitting data into training and testing sets...\")\n",
    "# Use the prepared X_encoded_df and Y\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_encoded_df, Y, test_size=0.3, random_state=42)\n",
    "print(f\"X_train shape: {X_train.shape}, Y_train shape: {Y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, Y_test shape: {Y_test.shape}\")\n",
    "\n",
    "\n",
    "# Optional: Clean up memory if the original combined_df is very large and no longer needed in full\n",
    "# import gc\n",
    "# del combined_df\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0134cff-e148-4516-b883-f96fae6f404d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a 20% sample of the data...\n",
      "Sampled data shape: X=(41243, 33), Y=(41243,)\n",
      "Sample split: X_train=(28870, 33), X_test=(12373, 33)\n"
     ]
    }
   ],
   "source": [
    "# --- Create a smaller sample ---\n",
    "# Adjust frac (e.g., 0.1 for 10%, 0.2 for 20%) based on your memory\n",
    "sample_fraction = 0.2\n",
    "print(f\"Creating a {sample_fraction*100:.0f}% sample of the data...\")\n",
    "\n",
    "# Sample the encoded features and the target variable\n",
    "X_encoded_sample = X_encoded_df.sample(frac=sample_fraction, random_state=42)\n",
    "Y_sample = Y.loc[X_encoded_sample.index] # Ensure Y matches the sampled X indices\n",
    "\n",
    "print(f\"Sampled data shape: X={X_encoded_sample.shape}, Y={Y_sample.shape}\")\n",
    "\n",
    "# --- Split the SAMPLE into training and testing sets ---\n",
    "X_train_sample, X_test_sample, Y_train_sample, Y_test_sample = train_test_split(\n",
    "    X_encoded_sample, Y_sample, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Sample split: X_train={X_train_sample.shape}, X_test={X_test_sample.shape}\")\n",
    "\n",
    "# --- NOW, re-run the model fitting code using these _sample variables ---\n",
    "# e.g., lasso_cv_model.fit(X_train_sample, Y_train_sample)\n",
    "# e.g., rf_model.fit(X_train_sample, Y_train_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1847ea-184d-498f-8740-e096c4ad4bd8",
   "metadata": {},
   "source": [
    "### Multi-linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d26513b-055c-487b-822c-3c8412851ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Run this in your Jupyter Notebook)\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# --- Linear Regression ---\n",
    "\n",
    "# Initialize the model\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "# --- K-Fold Cross-Validation on Training Data ---\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "cv_mse_scores = cross_val_score(lr_model, X_train_sample, Y_train_sample, cv=kf, scoring='neg_mean_squared_error', n_jobs=-1) # Use n_jobs=-1 for potentially faster CV\n",
    "cv_r2_scores = cross_val_score(lr_model, X_train_sample, Y_train_sample, cv=kf, scoring='r2', n_jobs=-1)\n",
    "\n",
    "#print(\"--- Linear Regression (Cross-Validation on Training Data) ---\")\n",
    "print(f\"Mean CV Negative MSE: {np.mean(cv_mse_scores):.4f} (+/- {np.std(cv_mse_scores):.4f})\")\n",
    "print(f\"Mean CV MSE: {-np.mean(cv_mse_scores):.4f}\")\n",
    "print(f\"Mean CV R-squared: {np.mean(cv_r2_scores):.4f} (+/- {np.std(cv_r2_scores):.4f})\")\n",
    "\n",
    "# --- Training on Full Training Set & Evaluation on Test Set ---\n",
    "lr_model.fit(X_train_sample, Y_train_sample)\n",
    "Y_pred_lr = lr_model.predict(X_test)\n",
    "mse_lr = mean_squared_error(Y_test, Y_pred_lr)\n",
    "mae_lr = mean_absolute_error(Y_test, Y_pred_lr)\n",
    "r2_lr = r2_score(Y_test, Y_pred_lr)\n",
    "\n",
    "print(\"\\n--- Linear Regression (Evaluation on Test Data) ---\")\n",
    "print(f\"Test MSE: {mse_lr:.4f}\")\n",
    "print(f\"Test MAE: {mae_lr:.4f}\")\n",
    "print(f\"Test R-squared: {r2_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff55cd3-63cc-456c-9013-5c55b812bf00",
   "metadata": {},
   "source": [
    "### Ridge and Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ec5fe5-163f-4a32-ac74-5446c346d0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
